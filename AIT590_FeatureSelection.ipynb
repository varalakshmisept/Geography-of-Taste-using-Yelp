{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the csv file created from DataCollection.ipynb to extract features. \n",
    "1. Data Pre-processing\n",
    "2. Lemmatization\n",
    "3. Extract top 1000 frequently used words\n",
    "4. Implement word2vec model\n",
    "5. Binarize the words in the reviews based on features selected\n",
    "6. Aggregate the features for each restaurant\n",
    "7. Apply Tf-IDF\n",
    "8. Save the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7_1AWuRXGQVm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kIrORtaTGcOs"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/varal/OneDrive - George Mason University/Desktop/MS/AIT 590/Project/AIT590-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VSOE6dEIlyd",
    "outputId": "43cb6d07-4e49-45f6-f8e6-08164d3211a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'city', 'postal_code', 'latitude', 'longitude',\n",
       "       'Restaurant Rating', 'review_count', 'user_id', 'review_id',\n",
       "       'User-RestaurantRating', 'text', 'date', 'user-reviewCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkB0_L-QInGr",
    "outputId": "cc6ad682-5d65-445f-92ad-5c1d0c9cd391"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30199, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "6ZLkey5AIqBd",
    "outputId": "92091f05-e111-4c75-ff73-c00c095a7b21"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Restaurant Rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>User-RestaurantRating</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user-reviewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utIA0LyQmwP-9DRyxUe6qQ</td>\n",
       "      <td>Snooze, An A.M. Eatery</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.508204</td>\n",
       "      <td>-112.037033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3515</td>\n",
       "      <td>h0fffFM3GcXll6FsgUGC8g</td>\n",
       "      <td>F6MW-SXeUw4P_NpSRufCpQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Great breakfast joint! \\n\\nHave been to Snooze...</td>\n",
       "      <td>1/4/2015 4:11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>utIA0LyQmwP-9DRyxUe6qQ</td>\n",
       "      <td>Snooze, An A.M. Eatery</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.508204</td>\n",
       "      <td>-112.037033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3515</td>\n",
       "      <td>3cC726zwgerKNNasnidAww</td>\n",
       "      <td>71Je4Eb7kX9vhvbrXsLgRw</td>\n",
       "      <td>4</td>\n",
       "      <td>I should have had the pancakes.\\n\\nDon't get m...</td>\n",
       "      <td>4/19/2014 21:39</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utIA0LyQmwP-9DRyxUe6qQ</td>\n",
       "      <td>Snooze, An A.M. Eatery</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.508204</td>\n",
       "      <td>-112.037033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3515</td>\n",
       "      <td>dEk7mXM4npuDPmxrwPcEgQ</td>\n",
       "      <td>Bx8IxSpyzeZcMQlmVa5hCQ</td>\n",
       "      <td>3</td>\n",
       "      <td>95 minute wait for breakfast? Really? So here ...</td>\n",
       "      <td>11/30/2014 23:45</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utIA0LyQmwP-9DRyxUe6qQ</td>\n",
       "      <td>Snooze, An A.M. Eatery</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.508204</td>\n",
       "      <td>-112.037033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3515</td>\n",
       "      <td>771OWzbzelsEeSlx8QsfsQ</td>\n",
       "      <td>egxw1AeFUURKuCZKjBP9XA</td>\n",
       "      <td>2</td>\n",
       "      <td>I so wanted to fall in love with the cuteness ...</td>\n",
       "      <td>12/7/2013 20:25</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>utIA0LyQmwP-9DRyxUe6qQ</td>\n",
       "      <td>Snooze, An A.M. Eatery</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>85016</td>\n",
       "      <td>33.508204</td>\n",
       "      <td>-112.037033</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3515</td>\n",
       "      <td>_jYEC7fvqTxu5R2jhk_NDQ</td>\n",
       "      <td>6ZgMgiayplF_kA-gBKqj2A</td>\n",
       "      <td>4</td>\n",
       "      <td>OMG, this place is so so so so so yummy...and ...</td>\n",
       "      <td>4/19/2014 18:05</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                    name     city  postal_code  \\\n",
       "0  utIA0LyQmwP-9DRyxUe6qQ  Snooze, An A.M. Eatery  Phoenix        85016   \n",
       "1  utIA0LyQmwP-9DRyxUe6qQ  Snooze, An A.M. Eatery  Phoenix        85016   \n",
       "2  utIA0LyQmwP-9DRyxUe6qQ  Snooze, An A.M. Eatery  Phoenix        85016   \n",
       "3  utIA0LyQmwP-9DRyxUe6qQ  Snooze, An A.M. Eatery  Phoenix        85016   \n",
       "4  utIA0LyQmwP-9DRyxUe6qQ  Snooze, An A.M. Eatery  Phoenix        85016   \n",
       "\n",
       "    latitude   longitude  Restaurant Rating  review_count  \\\n",
       "0  33.508204 -112.037033                4.0          3515   \n",
       "1  33.508204 -112.037033                4.0          3515   \n",
       "2  33.508204 -112.037033                4.0          3515   \n",
       "3  33.508204 -112.037033                4.0          3515   \n",
       "4  33.508204 -112.037033                4.0          3515   \n",
       "\n",
       "                  user_id               review_id  User-RestaurantRating  \\\n",
       "0  h0fffFM3GcXll6FsgUGC8g  F6MW-SXeUw4P_NpSRufCpQ                      5   \n",
       "1  3cC726zwgerKNNasnidAww  71Je4Eb7kX9vhvbrXsLgRw                      4   \n",
       "2  dEk7mXM4npuDPmxrwPcEgQ  Bx8IxSpyzeZcMQlmVa5hCQ                      3   \n",
       "3  771OWzbzelsEeSlx8QsfsQ  egxw1AeFUURKuCZKjBP9XA                      2   \n",
       "4  _jYEC7fvqTxu5R2jhk_NDQ  6ZgMgiayplF_kA-gBKqj2A                      4   \n",
       "\n",
       "                                                text              date  \\\n",
       "0  Great breakfast joint! \\n\\nHave been to Snooze...     1/4/2015 4:11   \n",
       "1  I should have had the pancakes.\\n\\nDon't get m...   4/19/2014 21:39   \n",
       "2  95 minute wait for breakfast? Really? So here ...  11/30/2014 23:45   \n",
       "3  I so wanted to fall in love with the cuteness ...   12/7/2013 20:25   \n",
       "4  OMG, this place is so so so so so yummy...and ...   4/19/2014 18:05   \n",
       "\n",
       "   user-reviewCount  \n",
       "0                30  \n",
       "1                25  \n",
       "2                48  \n",
       "3                26  \n",
       "4                23  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur7HHqePIv5G"
   },
   "source": [
    "**Data Pre-processing**    \n",
    "Pre-processing the reviews\n",
    "1. Converting to lower case\n",
    "2. Removing stop words, punctuations, digits\n",
    "3. Lemmatizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cc1G-BXNJ7nP"
   },
   "outputs": [],
   "source": [
    "def Preprocess(text):\n",
    "  # Remove Numbers\n",
    "  text = re.sub(r'[0-9]+', '', text)\n",
    "  # convert to lower case\n",
    "  text = text.lower()\n",
    "  # Remove punctuations and empty spaces\n",
    "  text = re.sub(r'[^a-zA-Z]',' ', text) \n",
    "  STOP_WORDS = stopwords.words('english')\n",
    "  # word tokenization and remove stop words\n",
    "  text = [w for w in text.split() if w not in STOP_WORDS]\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jYVkfd8AJ9iw"
   },
   "outputs": [],
   "source": [
    "#Reviews data\n",
    "text = data['text']\n",
    "text = text.apply(lambda x: Preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AfgR17pwKAxu"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def lemmatize_token(tokens):\n",
    "    tags = defaultdict(lambda : wn.NOUN)\n",
    "    tags['J'] = wn.ADJ\n",
    "    tags['V'] = wn.VERB\n",
    "    tags['R'] = wn.ADV\n",
    "\n",
    "    lemmitizer = WordNetLemmatizer()\n",
    "    new_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemmitizer.lemmatize(token, tags[tag[0]])\n",
    "        new_tokens.append(lemma)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oSDBLVZeNhsV"
   },
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: lemmatize_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "moQ8xeryODsX"
   },
   "outputs": [],
   "source": [
    "# Extracting top 1000 words used in the reviews\n",
    "newText = text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AhhbIHTVP1XE"
   },
   "outputs": [],
   "source": [
    "newText = ' '.join(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2v-DmT6oP2fi",
    "outputId": "d68f8746-1834-4429-83a8-0e558f881e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 31558), ('get', 28432), ('food', 24150), ('place', 24004), ('go', 23362), ('order', 21681), ('like', 21601), ('come', 20058), ('time', 18821), ('great', 18263), ('one', 17814), ('really', 15383), ('make', 15303), ('try', 15146), ('well', 13998), ('also', 13237), ('would', 13118), ('service', 12850), ('back', 12586), ('love', 12127), ('restaurant', 12020), ('menu', 11825), ('wait', 10348), ('say', 10228), ('table', 9872), ('drink', 9640), ('think', 9385), ('cheese', 9374), ('fry', 9318), ('take', 9295), ('chicken', 9225), ('even', 9097), ('burger', 9089), ('u', 9042), ('little', 8979), ('delicious', 8925), ('nice', 8914), ('bar', 8602), ('sauce', 8544), ('want', 8112), ('eat', 8103), ('best', 7984), ('pretty', 7966), ('much', 7884), ('look', 7814), ('flavor', 7738), ('taste', 7721), ('dish', 7596), ('side', 7447), ('thing', 7445), ('give', 7304), ('meal', 7207), ('definitely', 7174), ('know', 7088), ('first', 7081), ('night', 7043), ('friend', 6901), ('dinner', 6724), ('could', 6672), ('salad', 6671), ('two', 6477), ('always', 6319), ('seat', 6223), ('price', 6107), ('enjoy', 6032), ('people', 5908), ('pizza', 5891), ('still', 5826), ('bit', 5787), ('lot', 5680), ('serve', 5672), ('experience', 5624), ('server', 5622), ('right', 5621), ('way', 5603), ('star', 5599), ('lunch', 5592), ('top', 5542), ('see', 5531), ('sweet', 5530), ('beer', 5508), ('find', 5433), ('day', 5425), ('hour', 5414), ('favorite', 5379), ('though', 5329), ('fresh', 5323), ('small', 5319), ('visit', 5220), ('taco', 5219), ('dessert', 5054), ('since', 5045), ('everything', 4971), ('sure', 4920), ('happy', 4875), ('meat', 4875), ('bread', 4821), ('sandwich', 4817), ('around', 4778), ('big', 4767), ('never', 4754), ('room', 4721), ('next', 4720), ('ask', 4718), ('pork', 4658), ('sit', 4649), ('review', 4626), ('potato', 4551), ('long', 4485), ('start', 4408), ('friendly', 4390), ('special', 4326), ('egg', 4288), ('many', 4276), ('check', 4254), ('need', 4253), ('something', 4206), ('tasty', 4199), ('seem', 4167), ('perfect', 4125), ('option', 4096), ('minute', 4073), ('another', 4017), ('open', 3990), ('last', 3989), ('area', 3981), ('bad', 3947), ('brunch', 3940), ('new', 3936), ('feel', 3935), ('large', 3921), ('ever', 3897), ('spot', 3876), ('cream', 3829), ('hot', 3826), ('steak', 3797), ('enough', 3787), ('amaze', 3773), ('end', 3765), ('different', 3753), ('breakfast', 3748), ('cook', 3726), ('plate', 3725), ('item', 3699), ('spicy', 3651), ('every', 3629), ('roll', 3612), ('huge', 3598), ('portion', 3542), ('beef', 3522), ('super', 3517), ('shrimp', 3516), ('worth', 3511), ('tell', 3507), ('overall', 3490), ('recommend', 3476), ('line', 3466), ('bacon', 3464), ('walk', 3455), ('appetizer', 3440), ('work', 3436), ('leave', 3433), ('awesome', 3426), ('decide', 3424), ('staff', 3385), ('probably', 3361), ('bring', 3329), ('quite', 3316), ('use', 3313), ('location', 3277), ('full', 3262), ('atmosphere', 3204), ('however', 3192), ('expect', 3126), ('share', 3111), ('inside', 3104), ('add', 3053), ('wine', 3052), ('nothing', 3051), ('bite', 3033), ('excellent', 3021), ('offer', 3004), ('buffet', 2928), ('outside', 2923), ('chocolate', 2913), ('year', 2895), ('strip', 2894), ('rice', 2871), ('maybe', 2869), ('chip', 2868), ('fan', 2868), ('selection', 2855), ('actually', 2854), ('park', 2845), ('reservation', 2829), ('kind', 2824), ('stop', 2824), ('ice', 2814), ('din', 2812), ('onion', 2808), ('busy', 2805), ('vega', 2802), ('half', 2779), ('patio', 2776), ('cool', 2733), ('house', 2730), ('green', 2724), ('high', 2721), ('ok', 2712), ('sushi', 2690), ('fish', 2665), ('quality', 2658), ('light', 2655), ('soup', 2644), ('crispy', 2588), ('cocktail', 2568), ('choice', 2567), ('let', 2565), ('away', 2558), ('three', 2552), ('keep', 2542), ('coffee', 2534), ('entree', 2516), ('crab', 2509), ('tomato', 2471), ('red', 2462), ('group', 2447), ('home', 2425), ('husband', 2417), ('perfectly', 2414), ('anything', 2406), ('size', 2404), ('finally', 2404), ('everyone', 2399), ('noodle', 2393), ('put', 2375), ('course', 2367), ('waitress', 2361), ('fun', 2344), ('hard', 2333), ('rib', 2331), ('couple', 2328), ('week', 2326), ('point', 2310), ('cake', 2309), ('corn', 2294), ('without', 2281), ('fill', 2280), ('list', 2273), ('grill', 2272), ('arrive', 2248), ('include', 2227), ('stay', 2226), ('must', 2220), ('bean', 2218), ('almost', 2217), ('pay', 2204), ('part', 2197), ('waiter', 2171), ('return', 2158), ('oh', 2149), ('free', 2147), ('else', 2139), ('water', 2137), ('especially', 2115), ('butter', 2114), ('old', 2113), ('party', 2104), ('call', 2103), ('salsa', 2088), ('yelp', 2066), ('usually', 2065), ('yes', 2065), ('pm', 2065), ('kitchen', 2058), ('decent', 2033), ('mac', 2025), ('second', 2023), ('french', 2018), ('show', 2008), ('waffle', 1992), ('least', 1990), ('fantastic', 1987), ('style', 1981), ('close', 1975), ('choose', 1965), ('town', 1953), ('short', 1952), ('hotel', 1952), ('may', 1927), ('able', 1919), ('yet', 1918), ('far', 1910), ('although', 1897), ('warm', 1894), ('toast', 1884), ('flavorful', 1883), ('amazing', 1881), ('roast', 1879), ('dip', 1879), ('space', 1877), ('crowd', 1876), ('bowl', 1870), ('stuff', 1861), ('piece', 1857), ('glass', 1855), ('slice', 1853), ('bbq', 1852), ('tender', 1842), ('name', 1839), ('salmon', 1827), ('okay', 1820), ('wish', 1798), ('whole', 1793), ('mean', 1789), ('veggie', 1789), ('street', 1777), ('either', 1769), ('decor', 1768), ('absolutely', 1768), ('sunday', 1761), ('chef', 1758), ('white', 1740), ('pasta', 1739), ('amount', 1738), ('finish', 1737), ('door', 1731), ('do', 1724), ('saturday', 1708), ('local', 1700), ('pepper', 1696), ('thai', 1696), ('wrong', 1693), ('pack', 1666), ('mushroom', 1665), ('hand', 1657), ('dry', 1651), ('plus', 1649), ('live', 1647), ('might', 1646), ('several', 1646), ('weekend', 1631), ('cut', 1630), ('la', 1627), ('instead', 1620), ('change', 1620), ('type', 1613), ('date', 1612), ('stand', 1603), ('front', 1602), ('less', 1600), ('felt', 1593), ('reason', 1588), ('late', 1581), ('pick', 1581), ('season', 1580), ('miss', 1579), ('bun', 1579), ('dog', 1568), ('sausage', 1567), ('watch', 1566), ('ramen', 1565), ('wonderful', 1564), ('person', 1560), ('yummy', 1550), ('quickly', 1550), ('mexican', 1549), ('attentive', 1527), ('casino', 1515), ('cold', 1513), ('seafood', 1513), ('deal', 1508), ('head', 1508), ('bartender', 1504), ('remember', 1496), ('quick', 1490), ('extra', 1488), ('tea', 1487), ('view', 1487), ('fine', 1479), ('garlic', 1469), ('help', 1463), ('already', 1461), ('salt', 1458), ('hit', 1455), ('family', 1445), ('guy', 1442), ('regular', 1439), ('plenty', 1433), ('variety', 1431), ('fast', 1431), ('spice', 1429), ('crust', 1428), ('early', 1421), ('smoke', 1416), ('main', 1416), ('unique', 1413), ('split', 1413), ('soon', 1411), ('lobster', 1410), ('soft', 1404), ('creamy', 1401), ('ingredient', 1390), ('clean', 1382), ('pancake', 1376), ('set', 1370), ('someone', 1363), ('medium', 1359), ('disappoint', 1358), ('wing', 1345), ('downtown', 1342), ('easy', 1330), ('birthday', 1326), ('margarita', 1322), ('later', 1321), ('four', 1314), ('guess', 1299), ('mind', 1296), ('customer', 1291), ('base', 1290), ('pie', 1287), ('dress', 1277), ('move', 1275), ('city', 1273), ('fact', 1270), ('salty', 1268), ('floor', 1268), ('belly', 1264), ('glad', 1261), ('friday', 1261), ('note', 1257), ('beautiful', 1256), ('care', 1256), ('rather', 1252), ('game', 1251), ('run', 1250), ('talk', 1249), ('hear', 1247), ('past', 1243), ('ate', 1236), ('tuna', 1229), ('pittsburgh', 1229), ('average', 1226), ('believe', 1224), ('surprise', 1223), ('slow', 1221), ('music', 1216), ('highly', 1215), ('phoenix', 1214), ('trip', 1214), ('mention', 1213), ('solid', 1211), ('together', 1204), ('ambiance', 1202), ('biscuit', 1200), ('bottle', 1199), ('black', 1197), ('ordered', 1189), ('mouth', 1188), ('level', 1184), ('vegas', 1180), ('real', 1178), ('along', 1177), ('hostess', 1170), ('oyster', 1169), ('wow', 1167), ('n', 1165), ('saw', 1164), ('texture', 1164), ('turn', 1161), ('sound', 1155), ('juicy', 1151), ('charlotte', 1149), ('available', 1143), ('tip', 1142), ('cheap', 1138), ('wife', 1135), ('loud', 1127), ('disappointed', 1127), ('lack', 1122), ('consider', 1111), ('etc', 1109), ('read', 1099), ('today', 1093), ('mix', 1091), ('ago', 1089), ('wall', 1085), ('avocado', 1085), ('fried', 1082), ('kid', 1081), ('meet', 1080), ('shake', 1080), ('tacos', 1072), ('locate', 1068), ('truffle', 1068), ('italian', 1067), ('yum', 1065), ('simple', 1062), ('lemon', 1061), ('rich', 1061), ('grab', 1059), ('extremely', 1055), ('write', 1054), ('morning', 1051), ('others', 1050), ('pickle', 1043), ('often', 1040), ('topping', 1040), ('vibe', 1038), ('entire', 1035), ('rest', 1034), ('happen', 1032), ('w', 1029), ('play', 1028), ('seriously', 1028), ('shop', 1026), ('thick', 1023), ('please', 1020), ('honestly', 1017), ('duck', 1012), ('impressed', 1011), ('meatball', 1011), ('boyfriend', 1010), ('ton', 1009), ('mixed', 1008), ('curry', 1007), ('vegetarian', 1006), ('club', 1003), ('totally', 1001), ('ready', 999), ('parking', 995), ('lamb', 992), ('brisket', 991), ('joint', 989), ('expensive', 986), ('forget', 985), ('hungry', 984), ('pudding', 983), ('month', 982), ('vegetable', 981), ('plan', 980), ('broth', 975), ('manager', 973), ('girl', 968), ('outdoor', 965), ('sample', 965), ('stick', 963), ('tried', 963), ('unfortunately', 961), ('grit', 961), ('chili', 957), ('standard', 957), ('near', 956), ('heard', 955), ('prefer', 955), ('suggest', 954), ('bill', 952), ('excite', 952), ('immediately', 952), ('forward', 951), ('scallop', 951), ('drive', 948), ('diner', 947), ('pho', 946), ('guacamole', 945), ('cute', 944), ('melt', 942), ('five', 940), ('slightly', 939), ('rare', 934), ('money', 934), ('problem', 933), ('cup', 931), ('tortilla', 928), ('banana', 924), ('fruit', 920), ('hash', 918), ('bland', 917), ('business', 913), ('treat', 908), ('addition', 906), ('blue', 906), ('dark', 906), ('complaint', 901), ('issue', 893), ('thought', 892), ('counter', 890), ('strawberry', 890), ('world', 889), ('apple', 888), ('touch', 886), ('reasonable', 880), ('idea', 878), ('cleveland', 878), ('burrito', 877), ('pricey', 876), ('life', 873), ('chance', 872), ('man', 871), ('kick', 871), ('across', 871), ('due', 867), ('cover', 866), ('station', 865), ('refill', 864), ('wrap', 863), ('sprout', 863), ('receive', 862), ('literally', 860), ('heat', 860), ('casual', 859), ('crazy', 852), ('honey', 852), ('homemade', 851), ('brown', 850), ('pull', 847), ('asian', 845), ('recently', 842), ('chop', 842), ('sometimes', 841), ('combination', 841), ('pool', 841), ('hop', 840), ('eye', 837), ('goat', 835), ('thin', 834), ('notice', 833), ('concept', 832), ('sort', 831), ('summer', 829), ('event', 827), ('classic', 826), ('completely', 824), ('charge', 823), ('helpful', 817), ('th', 816), ('opt', 812), ('provide', 811), ('despite', 810), ('crisp', 809), ('easily', 807), ('crave', 803), ('mine', 803), ('incredible', 803), ('strong', 802), ('hope', 800), ('mash', 798), ('crepe', 798), ('certainly', 792), ('empty', 790), ('perfection', 790), ('job', 788), ('traditional', 787), ('number', 784), ('bone', 783), ('twice', 782), ('orange', 780), ('juice', 780), ('savory', 780), ('modern', 778), ('mussel', 774), ('can', 773), ('spinach', 773), ('tiny', 772), ('appreciate', 772), ('thank', 769), ('save', 766), ('combo', 764), ('benedict', 763), ('vegan', 763), ('hype', 762), ('nicely', 757), ('truly', 756), ('comfortable', 756), ('owner', 756), ('e', 755), ('become', 755), ('interest', 753), ('behind', 753), ('per', 751), ('prepare', 750), ('book', 749), ('oil', 749), ('compare', 748), ('cafe', 745), ('popular', 742), ('bathroom', 740), ('low', 739), ('spend', 735), ('simply', 734), ('heavy', 731), ('word', 730), ('rock', 730), ('valet', 730), ('healthy', 729), ('afternoon', 727), ('anyway', 726), ('upon', 726), ('face', 726), ('hold', 726), ('bake', 726), ('round', 725), ('sign', 725), ('guest', 724), ('recommendation', 723), ('opinion', 723), ('exactly', 721), ('anyone', 719), ('case', 718), ('gravy', 718), ('request', 718), ('lettuce', 718), ('turkey', 718), ('die', 717), ('middle', 715), ('basically', 713), ('welcome', 712), ('single', 711), ('peanut', 710), ('fancy', 710), ('greet', 709), ('impress', 708), ('fairly', 706), ('p', 706), ('cheddar', 705), ('goodness', 703), ('photo', 702), ('generous', 700), ('follow', 700), ('prime', 698), ('brew', 697), ('hang', 695), ('card', 695), ('update', 692), ('olive', 692), ('caramel', 690), ('smell', 688), ('jam', 687), ('window', 685), ('sat', 682), ('fat', 682), ('sugar', 681), ('lovely', 681), ('weird', 680), ('dining', 675), ('version', 670), ('select', 668), ('begin', 668), ('basil', 667), ('complimentary', 666), ('not', 665), ('bed', 664), ('rave', 663), ('crunchy', 659), ('expectation', 658), ('sour', 658), ('mom', 656), ('fall', 655), ('calamari', 655), ('moist', 654), ('total', 652), ('perhaps', 652), ('except', 649), ('bloody', 648), ('outstanding', 648), ('mostly', 647), ('interior', 642), ('milk', 641), ('queso', 641), ('figure', 639), ('build', 639), ('fire', 639), ('mimosa', 638), ('pair', 637), ('cost', 637), ('syrup', 637), ('lady', 637), ('occasion', 637), ('booth', 636), ('lime', 636), ('lol', 634), ('understand', 633), ('non', 633), ('typical', 632), ('cucumber', 631), ('realize', 629), ('pretzel', 629), ('feature', 628), ('pop', 628), ('prepared', 628), ('explain', 627), ('neighborhood', 627), ('vanilla', 626), ('pineapple', 626), ('boy', 625), ('filet', 624), ('coconut', 624), ('within', 621), ('whatever', 621), ('enjoyed', 621), ('rating', 621), ('caesar', 621), ('fabulous', 619), ('refresh', 618), ('sea', 617), ('center', 616), ('deep', 615), ('recent', 615), ('min', 614), ('throughout', 612), ('normally', 611), ('hate', 611), ('american', 609), ('true', 608), ('chorizo', 607), ('spring', 607), ('damn', 607), ('aioli', 606), ('picture', 604), ('st', 602), ('http', 602), ('upstairs', 602), ('alone', 601), ('blow', 601), ('balance', 601), ('establishment', 599), ('usual', 599), ('com', 599), ('tap', 599), ('tv', 598), ('corner', 596), ('steam', 594), ('yeah', 592), ('rush', 592), ('slaw', 592), ('spoon', 592), ('pleasant', 590), ('mary', 587), ('beat', 587), ('authentic', 587), ('pad', 587), ('section', 586), ('kinda', 585), ('patty', 585), ('slider', 585), ('similar', 584), ('bunch', 583), ('friends', 583), ('allow', 581), ('tuesday', 580), ('satisfy', 580), ('cozy', 580), ('heart', 580), ('double', 578), ('pastry', 578), ('tofu', 578), ('overly', 576), ('presentation', 576), ('shell', 573), ('original', 572), ('wood', 572), ('mango', 572), ('b', 571), ('market', 571), ('southern', 570), ('board', 568), ('shot', 568), ('thanks', 567), ('suppose', 567), ('donut', 567), ('app', 566), ('greasy', 565), ('trendy', 564), ('unless', 564), ('chair', 564), ('movie', 563), ('cheesecake', 563), ('dine', 561), ('box', 560), ('future', 559), ('interesting', 559), ('mustard', 558), ('break', 557), ('matter', 557), ('multiple', 556), ('delish', 554), ('question', 554), ('weather', 554), ('buy', 552), ('bottom', 552), ('craft', 550), ('describe', 549), ('continue', 549), ('gelato', 549), ('agree', 545), ('accommodate', 545), ('remind', 543), ('jalapeno', 542), ('flat', 541), ('de', 539), ('buck', 539), ('impressive', 537), ('parmesan', 537), ('guac', 536), ('monday', 535), ('sorry', 534), ('value', 532), ('host', 531), ('sister', 531), ('drop', 528), ('pass', 526), ('bellagio', 526), ('lucky', 524), ('mini', 524), ('tonight', 524), ('offering', 523), ('baby', 522), ('conversation', 521), ('limit', 521), ('deliver', 519), ('ring', 517), ('particularly', 517), ('present', 516), ('hip', 515), ('skip', 512), ('flatbread', 512), ('imagine', 511), ('ahead', 509), ('chinese', 509), ('cilantro', 509), ('otherwise', 507), ('cuisine', 505), ('hubby', 505), ('platter', 504), ('pan', 504), ('www', 503), ('mozzarella', 502), ('disappointing', 501), ('pot', 499), ('sad', 497), ('cinnamon', 497), ('incredibly', 497), ('risotto', 497), ('omelet', 496), ('complain', 496), ('south', 495), ('clear', 494), ('heaven', 493), ('chewy', 493), ('upscale', 491), ('lounge', 491), ('notch', 491), ('ball', 491), ('odd', 490), ('speak', 489), ('mood', 487), ('dumpling', 487), ('smile', 485), ('hell', 484), ('surprisingly', 484), ('somewhat', 483), ('car', 482), ('carrot', 481), ('separate', 481), ('step', 481), ('whip', 480), ('chunk', 480), ('east', 478), ('catch', 478), ('secret', 478), ('lose', 477), ('enter', 476), ('barely', 476), ('fair', 476), ('mayo', 475), ('personally', 475), ('stuffed', 475), ('avoid', 474), ('liked', 473), ('skin', 473), ('frozen', 473), ('normal', 472), ('store', 472), ('bf', 472), ('none', 471), ('meh', 471), ('difficult', 469), ('crunch', 469), ('cookie', 469), ('tot', 469), ('dollar', 465), ('ham', 464), ('venetian', 464), ('attention', 463), ('cause', 462), ('somewhere', 462), ('tough', 462), ('plain', 461), ('leftover', 461), ('anywhere', 460)]\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency Distribution:\n",
    "freq_dist = nltk.FreqDist(word_tokenize(newText))\n",
    "# top 1000 frequentwords\n",
    "print(freq_dist.most_common(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "NUEhvHNZQKYJ"
   },
   "outputs": [],
   "source": [
    "# List of selected features \n",
    "features = ['cheese', 'chicken', 'burger',  'sauce', 'salad', 'pizza', 'dessert', 'pork', 'egg', 'meat', 'steak', 'beef', 'shrimp', 'bacon', 'fish', 'bbq', 'pasta', 'thai', 'mexican', 'seafood', 'tea', 'italian', 'vegetarian', 'asian', 'goat', 'vegan', 'american', 'chinese','beer', 'wine', 'cocktail', 'shake', 'juice', 'coffee','fry', 'tasty', 'hot', 'spicy', 'crispy', 'grill', 'toast', 'roast', 'yummy', 'salt', 'spice', 'creamy', 'juicy', 'fried', 'crisp', 'bake', 'crunchy', 'sour', 'greasy', 'chewy','friendly', 'fantastic',  'classic', 'incredible', 'authentic', 'overly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laMDdlOnxu-w"
   },
   "source": [
    "WORD 2 vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RwbWn9hbxxPv"
   },
   "outputs": [],
   "source": [
    "text = text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "W1BUYzcuyKAs"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'review': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jVRbErZYyTka"
   },
   "outputs": [],
   "source": [
    "# phrases take list of list as input\n",
    "reviews = [row.split() for row in df['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VizICpu5yeFS"
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(reviews, min_count=30, progress_per=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uCYoo3ahyflV"
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h5MJqh-xzHiw"
   },
   "outputs": [],
   "source": [
    "# word2vec model implementation\n",
    "# Parameters to word2vec model\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "l1NJE0fgzZxD"
   },
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "w2v_model.build_vocab(sentences, progress_per=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "k04Ak83pzfk9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37363694, 78770070)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the word2vec Model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jmGWl4IMzosJ"
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yT8iYnrWz2vH"
   },
   "outputs": [],
   "source": [
    "# Generate features from the features selected from top 1000 words\n",
    "featuresGenerated = []\n",
    "i = 0\n",
    "for j in features:\n",
    "    # similar words generation\n",
    "    x = w2v_model.wv.most_similar(positive=[j])\n",
    "    y = []\n",
    "    i +=1\n",
    "    y.append(i)\n",
    "    y.append(j)\n",
    "    for k in x:\n",
    "        t = k[0]\n",
    "        y.append(t) \n",
    "    featuresGenerated.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures = ['chorizo', 'rib' , 'brisket', 'hanger_steak', 'filet', 'spencer', 'ribeye', 'skirt', 'tomahawk', 'filet','mignon',\n",
    "                'duck',  'meatball', 'lamb', 'turkey', 'calamari', 'rotisserie', 'breast', 'loin', 'angus', 'spare', 'shank', 'sheep',\n",
    "                'cream', 'chocolate', 'cake', 'waffle', 'biscuit', 'truffle', 'pudding', 'honey', 'crepe', 'caramel', 'jam', 'sugar', 'vanilla', 'pastry', 'cheesecake', 'gelato', 'cinnamon', 'cookie', 'chocolate_mousse', 'tiramisu', 'brownie', 'oreo', 'whipped', 'muffin', 'croissant',\n",
    "                'potato', 'onion', 'tomato', 'corn', 'bean', 'mushroom', 'garlic', 'lemon', 'pickle', 'vegetable', 'chili', 'spinach', 'lettuce', 'peanut', 'olive', 'basil', 'cucumber', 'jalapeno', 'cilantro', 'carrot', 'endive', 'green', 'portobello',\n",
    "                'avocado', 'banana', 'fruit', 'strawberry', 'apple', 'orange', 'pineapple', 'coconut', 'mango', 'papaya', 'kiwi', 'guava',\n",
    "                'margarita', 'ipas', 'hefeweizen', 'chianti', 'vino', 'malbec', 'pinot', 'sauvignon', 'negroni', 'gimlet', 'mojito', 'gin',\n",
    "                'salsa', 'pepper', 'guacamole', 'cheddar', 'aioli', 'parmesan', 'mozzarella', 'mayo', 'mayonnaise', 'marinara', 'fraiche',\n",
    "                'bread', 'pancake', 'pie', 'burrito', 'donut', 'flatbread', 'omelet', 'scramble', 'cereal',\n",
    "                'taco', 'sandwich', 'roll', 'rice', 'bun', 'dog', 'sausage', 'tortilla', 'savory', 'caesar', 'chowder', 'etouffee',\n",
    "                'pasta', 'risotto', 'dumpling', 'biancoverde', 'tagliatelle', 'carbonara', 'pappardelle', 'spaghetti', 'penne', 'bucatini', 'lasagna', 'fettuccine','linguini', 'spaghetti', 'spaetzle', 'focaccia',\n",
    "                'chip', 'pretzel', 'patty', 'cheeseburger', 'gourmet', 'delux', 'ronin', 'hamburger', 'pepperoni', 'french', 'parmesan', 'rotisserie', 'hashbrowns', 'croquette', 'tots',\n",
    "                'shrimp', 'crab', 'salmon', 'lobster', 'tuna', 'oyster', 'scallop', 'mussel', 'squid', 'tilapia', 'hamachi', 'sear', 'halibut', 'albacore', 'tilapia',\n",
    "                'sushi', 'noodle', 'ramen', 'tofu', 'kalbi', 'congee', 'dim',\n",
    "                'mimosa', 'lime', 'shot', 'milkshake',\n",
    "                'provolone', 'gouda', 'manchego', 'asiago', 'white', 'gruyere', 'cheddar', 'havarti', 'jack', 'mancheg',\n",
    "                'tinga', 'bean',\n",
    "                'cobb', 'argula', 'caesar',\n",
    "                'tom', 'panang','curry', 'massaman', 'kee', 'pad',\n",
    "                'milk', 'iced', 'lattes','french', 'matcha', 'cappuccino',\n",
    "                'taiwanese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalFeatures.extend(features)\n",
    "len(finalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 274 features are collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the number of words selected from finalFeatures where 1 word exist 0 not exists\n",
    "def binarize(finalFeatures, text):\n",
    "    tokens = word_tokenize(text)\n",
    "    binaryText = []\n",
    "    for item in finalFeatures:\n",
    "        if item in tokens:\n",
    "            binaryText.append(1)\n",
    "        else:\n",
    "            binaryText.append(0)\n",
    "    return binaryText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryText = text.apply(lambda x: binarize(finalFeatures, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sum(list(binaryText[2:3]),[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now each record is coded in terms of these 274 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30199, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBinaryText = pd.DataFrame({'review': binaryText})\n",
    "dfBinaryText.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBinaryText['business'] = data['business_id'] \n",
    "dfBinaryText['latitude'] = data['latitude']\n",
    "dfBinaryText['longitude'] = data['longitude']\n",
    "dfBinaryText['stars'] = data['Restaurant Rating']\n",
    "dfBinaryText['city'] = data['city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>36.168783</td>\n",
       "      <td>-115.139913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-050d_XIor1NpCuWkbIVaQ</td>\n",
       "      <td>33.456696</td>\n",
       "      <td>-112.072327</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-7H-oXvCxJzuT42ky6Db0g</td>\n",
       "      <td>40.470783</td>\n",
       "      <td>-79.960250</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01fuY2NNscttoTxOYbuZXw</td>\n",
       "      <td>35.229128</td>\n",
       "      <td>-80.867464</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>042IHd5KjHiMuBtGtugO_g</td>\n",
       "      <td>33.584063</td>\n",
       "      <td>-111.979760</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business   latitude   longitude  stars        city  \\\n",
       "5                  #NAME?  36.168783 -115.139913    4.0   Las Vegas   \n",
       "6  -050d_XIor1NpCuWkbIVaQ  33.456696 -112.072327    4.0     Phoenix   \n",
       "7  -7H-oXvCxJzuT42ky6Db0g  40.470783  -79.960250    3.5  Pittsburgh   \n",
       "8  01fuY2NNscttoTxOYbuZXw  35.229128  -80.867464    4.0   Charlotte   \n",
       "9  042IHd5KjHiMuBtGtugO_g  33.584063 -111.979760    4.0     Phoenix   \n",
       "\n",
       "                                              review  \n",
       "5  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "6  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "7  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "8  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "9  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregating the records(Reviews) of every restaurant, creating a row for each restaurant\n",
    "dfAggregate = (dfBinaryText['review'].groupby([dfBinaryText.business,dfBinaryText.latitude, dfBinaryText.longitude,dfBinaryText.stars,dfBinaryText.city]).apply(list)).to_frame().reset_index()\n",
    "dfAggregate[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAggregate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each record contains list of list, we are converting that into 1 list. That is combining all the vectors into one vector\n",
    "a = dfAggregate['review'].apply(lambda x: (sum(x,[])))\n",
    "dfAggregate1 = pd.DataFrame({'review':a})\n",
    "dfAggregate1['business'] = dfAggregate['business']\n",
    "dfAggregate1['stars'] = dfAggregate['stars']\n",
    "dfAggregate1['city'] = dfAggregate['city']\n",
    "dfAggregate1['latitude'] = dfAggregate['latitude']\n",
    "dfAggregate1['longitude'] = dfAggregate['longitude']\n",
    "dfAggregate = dfAggregate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data to Construct TF-IDF \n",
    "def convertToWord(text, finalFeatures):\n",
    "    wordForm = []\n",
    "    n = len(finalFeatures)\n",
    "    index = 0\n",
    "    for item in text:\n",
    "        if item == 1:\n",
    "            wordForm.append(finalFeatures[index])   \n",
    "        index +=1\n",
    "        if index == n:\n",
    "                index = 0\n",
    "    return wordForm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWord = dfAggregate['review'].apply(lambda x: convertToWord(x,finalFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWord = pd.DataFrame({'review':dfWord})\n",
    "dfWord['business'] = dfAggregate['business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWord[5:10]\n",
    "df = dfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = dfWord['review'].apply(lambda x: ' '.join(x))\n",
    "x = ' '.join(x)\n",
    "x = word_tokenize(x)\n",
    "x\n",
    "len(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>business</th>\n",
       "      <th>aioli</th>\n",
       "      <th>albacore</th>\n",
       "      <th>american</th>\n",
       "      <th>angus</th>\n",
       "      <th>apple</th>\n",
       "      <th>argula</th>\n",
       "      <th>asiago</th>\n",
       "      <th>asian</th>\n",
       "      <th>...</th>\n",
       "      <th>vanilla</th>\n",
       "      <th>vegan</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>vino</th>\n",
       "      <th>waffle</th>\n",
       "      <th>whipped</th>\n",
       "      <th>white</th>\n",
       "      <th>wine</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[rib, potato, onion, tomato, garlic, vegetable...</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>0.016088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031826</td>\n",
       "      <td>0.014518</td>\n",
       "      <td>0.012627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[waffle, egg, bacon, american, fry, waffle, ba...</td>\n",
       "      <td>-050d_XIor1NpCuWkbIVaQ</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017481</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.311309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[potato, ipas, bread, flatbread, sandwich, chi...</td>\n",
       "      <td>-7H-oXvCxJzuT42ky6Db0g</td>\n",
       "      <td>0.086986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.038104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.049431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[potato, chili, mayo, cheese, salad, vegan, be...</td>\n",
       "      <td>01fuY2NNscttoTxOYbuZXw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009751</td>\n",
       "      <td>0.063366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.310072</td>\n",
       "      <td>0.017208</td>\n",
       "      <td>0.153748</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[skirt, garlic, lemon, green, avocado, aioli, ...</td>\n",
       "      <td>042IHd5KjHiMuBtGtugO_g</td>\n",
       "      <td>0.100976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review                business  \\\n",
       "5  [rib, potato, onion, tomato, garlic, vegetable...                  #NAME?   \n",
       "6  [waffle, egg, bacon, american, fry, waffle, ba...  -050d_XIor1NpCuWkbIVaQ   \n",
       "7  [potato, ipas, bread, flatbread, sandwich, chi...  -7H-oXvCxJzuT42ky6Db0g   \n",
       "8  [potato, chili, mayo, cheese, salad, vegan, be...  01fuY2NNscttoTxOYbuZXw   \n",
       "9  [skirt, garlic, lemon, green, avocado, aioli, ...  042IHd5KjHiMuBtGtugO_g   \n",
       "\n",
       "      aioli  albacore  american  angus     apple    argula  asiago     asian  \\\n",
       "5  0.000000       0.0  0.000000    0.0  0.000000  0.000000     0.0  0.020644   \n",
       "6  0.000000       0.0  0.015693    0.0  0.016032  0.000000     0.0  0.000000   \n",
       "7  0.086986       0.0  0.027238    0.0  0.027826  0.038104     0.0  0.000000   \n",
       "8  0.000000       0.0  0.008753    0.0  0.000000  0.000000     0.0  0.020777   \n",
       "9  0.100976       0.0  0.000000    0.0  0.000000  0.000000     0.0  0.000000   \n",
       "\n",
       "   ...   vanilla     vegan  vegetable  vegetarian  vino    waffle   whipped  \\\n",
       "5  ...  0.000000  0.010494   0.016088    0.000000   0.0  0.000000  0.000000   \n",
       "6  ...  0.017481  0.018934   0.000000    0.013976   0.0  0.311309  0.000000   \n",
       "7  ...  0.000000  0.016432   0.000000    0.012129   0.0  0.000000  0.000000   \n",
       "8  ...  0.009751  0.063366   0.000000    0.109139   0.0  0.310072  0.017208   \n",
       "9  ...  0.000000  0.000000   0.000000    0.000000   0.0  0.000000  0.000000   \n",
       "\n",
       "      white      wine     yummy  \n",
       "5  0.031826  0.014518  0.012627  \n",
       "6  0.022970  0.000000  0.045567  \n",
       "7  0.009967  0.034100  0.049431  \n",
       "8  0.153748  0.021916  0.025416  \n",
       "9  0.000000  0.065972  0.000000  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF \n",
    "temp = dfWord['review'].apply(lambda x: ' '.join(x))\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(temp)\n",
    "# converting the data to dataframe\n",
    "df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "resultTfIDF = pd.concat([dfWord, df1], axis=1)\n",
    "resultTfIDF[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTfIDF['stars'] = dfAggregate['stars']\n",
    "resultTfIDF['city'] = dfAggregate['city']\n",
    "resultTfIDF['latitude'] = dfAggregate['latitude']\n",
    "resultTfIDF['longitude'] = dfAggregate['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 264)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultTfIDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultTfIDF.to_csv('FinalFeatures1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AIT590_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
